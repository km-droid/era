Repository layout
repo-root/
├─ .github/
│  └─ workflows/
│     ├─ terraform-plan.yml
│     └─ terraform-apply.yml
├─ infra/
│  ├─ modules/
│  │  └─ pubsub_stack/
│  │     ├─ main.tf
│  │     ├─ variables.tf
│  │     └─ versions.tf
│  ├─ shard-0/
│  │  ├─ backend.tf
│  │  ├─ main.tf
│  │  ├─ terraform.tfvars
│  │  └─ versions.tf
│  ├─ shard-1/
│  │  ├─ backend.tf
│  │  ├─ main.tf
│  │  ├─ terraform.tfvars
│  │  └─ versions.tf
│  ├─ shard-2/   (same four files)
│  ├─ shard-3/   (same four files)
│  ├─ shard-4/   (same four files)
│  └─ shard-5/   (same four files)
├─ config/
│  └─ stores.json
├─ .gitignore
└─ README.md

config/stores.json (IDs source)
["000001","000002","000003"]


You can use CSV instead if you prefer; the module supports both.

infra/modules/pubsub_stack/variables.tf
variable "project_id"   { type = string }
variable "shard_count"  { type = number  default = 6 }
variable "shard_index"  { type = number }          # 0..(shard_count-1)
variable "input_format" { type = string  default = "json" } # "json" or "csv"
variable "stores_file"  { type = string  default = "${path.root}/../../config/stores.json" }

# Pub/Sub knobs you may want to expose
variable "message_retention" { type = string default = "604800s" } # 7 days
variable "subscription_labels" {
  type    = map(string)
  default = { managed_by = "terraform" }
}
variable "subscription_options" {
  type = object({
    filters = optional(string, null) # align with your module's schema
  })
  default = { filters = null }
}

infra/modules/pubsub_stack/versions.tf
terraform {
  required_version = ">= 1.5.0"
  required_providers {
    google = { source = "hashicorp/google", version = ">= 5.0.0" }
  }
}

infra/modules/pubsub_stack/main.tf
locals {
  # Load store IDs (JSON or CSV)
  store_ids_json = var.input_format == "json" ? jsondecode(file(var.stores_file)) : []
  store_rows_csv = var.input_format == "csv"  ? csvdecode(file(var.stores_file))  : []
  store_ids_csv  = [for r in local.store_rows_csv : r.id]
  store_ids      = var.input_format == "json" ? local.store_ids_json : local.store_ids_csv

  # Deterministic sharding by hash(id)
  shard_filtered_ids = [
    for id in local.store_ids :
    id if (parseint(substr(md5(id), 0, 8), 16) % var.shard_count) == var.shard_index
  ]

  # Build resource names
  stores = {
    for id in local.shard_filtered_ids :
    id => {
      topic_name = "store-topic-${id}"
      sub_name   = "store-sub-${id}"
    }
  }
}

# One pubsub module per store ID
module "store_pubsub" {
  source  = "git::https://github.com/platform-org/modules.git//pubsub?ref=refs/tags/v3.0"
  for_each = local.stores

  project_id                 = var.project_id
  name                       = each.value.topic_name
  # NOTE: keep this spelling exactly as your external module expects.
  # If the real variable is "message_retention_dutation" (typo), mirror it:
  message_retention_dutation = var.message_retention

  subscription_project_id = var.project_id

  subscriptions = {
    "${each.value.sub_name}" = {
      # Align these keys to your module's input schema ("labels" vs "lables", "filter" vs "filters").
      lables  = var.subscription_labels
      options = { filters = var.subscription_options.filters }
    }
  }
}

Shard folders (repeat for shard-0..shard-5)
infra/shard-0/backend.tf
terraform {
  backend "gcs" {
    bucket = "YOUR_TF_STATE_BUCKET"
    prefix = "pubsub/shard-0"   # unique per shard
  }
}

infra/shard-0/versions.tf
terraform {
  required_version = ">= 1.5.0"
}

infra/shard-0/main.tf
module "stack" {
  source       = "../modules/pubsub_stack"
  project_id   = var.project_id
  shard_index  = 0
  shard_count  = 6

  # Optional overrides
  # input_format       = "json"
  # stores_file        = "${path.root}/../../config/stores.json"
  # message_retention  = "604800s"
  # subscription_labels = { managed_by = "terraform", env = "prod" }
  # subscription_options = { filters = "blah" }
}

variable "project_id" { type = string }

infra/shard-0/terraform.tfvars
project_id = "gcp-your-project-id"


For shards 1–5, copy the same four files, change prefix in backend.tf and the shard_index in main.tf to 1..5.

.gitignore
**/.terraform/*
**/.terraform.lock.hcl
*.tfplan
*.tfstate
*.tfstate.backup
.terraform.tfstate.lock.info

README.md (excerpt)
# Pub/Sub at Scale with Terraform Shards

We manage ~6,000 topics + 6,000 subscriptions using 6 Terraform shards (≈1k resources each).
Shards are selected deterministically by hashing the store ID, so each ID always maps to the same shard.

## Layout
- `config/stores.json` — authoritative list of store IDs (or a CSV with header `id`).
- `infra/modules/pubsub_stack` — shared Terraform root module with sharding logic.
- `infra/shard-N` — thin wrappers per shard with their own backend + variables.

## Workflows
- PRs: `.github/workflows/terraform-plan.yml` runs `plan` for all shards in parallel.
- After merge to `main`: `.github/workflows/terraform-apply.yml` runs `apply` for all shards.

## Local runs (optional)
```bash
cd infra/shard-0 && terraform init && terraform plan -parallelism=50

Adding/Removing Stores

Append/remove IDs in config/stores.json.

Open a PR. On merge, only the shard(s) owning those IDs will change.


---

### Notes & options

- **Single source of truth**: one `config/stores.json` (or CSV). The module reads it; each shard filters its slice.
- **Backend isolation**: each shard has a unique `backend.tf` `prefix` to avoid lock contention.
- **Module variable names**: ensure the keys you pass to your **external pubsub module** match its expectations (`lables` vs `labels`, `filters` vs `filter`, retention var name). I mirrored your snippet; adjust if needed.
- **Policy guardrails**: keep `prevent_destroy` inside your external module defaults, and expose a toggle var to intentionally permit deletes during controlled PRs.
- **IAM**: if you attach large IAM bindings, consider a separate module/state per shard to keep applies snappy.

If you want, I can paste ready-to-use **workflow YAMLs** wired to GCP Workload Identity (with matrix over shards) customized to your bucket name and TF SA.
