//
monitoring/
├─ terraform/
│  ├─ main.tf
│  ├─ variables.tf
│  └─ terraform.tfvars.example
├─ grafana/
│  ├─ dashboards/
│  │  └─ pubsub-top10-promql.json
│  └─ provisioning/
│     └─ datasources/
│        └─ prometheus-gmp.yaml
└─ promql/
   └─ pubsub.promql


/////

terraform/terraform.tfvars.example
project_id = "my-gcp-project-id"

notification_channels = [
  "channels/1234567890123",
  "channels/9876543210987"
]

# topic_regex        = "store-[0-9]+-topic"
# subscription_regex = "store-[0-9]+-sub"

# threshold_publish_failure_pct   = 1.0
# threshold_publish_p95_latency_s = 2.0
# threshold_backlog               = 150000
# threshold_oldest_age_s          = 1200
# threshold_ack_rate_min          = 0.2
# threshold_service_5xx_pct       = 1.0
# threshold_quota_utilization_pct = 80.0

//variables.tf

variable "project_id" {
  type        = string
  description = "GCP project id."
}

variable "notification_channels" {
  type        = list(string)
  description = "Monitoring channel resource names (e.g., channels/123...)."
  default     = []
}

variable "topic_regex" {
  type        = string
  default     = "store-[0-9]+-topic"
  description = "Regex for topics to include."
}

variable "subscription_regex" {
  type        = string
  default     = "store-[0-9]+-sub"
  description = "Regex for subscriptions to include."
}

variable "threshold_publish_failure_pct" {
  type        = number
  default     = 1.0
}

variable "threshold_publish_p95_latency_s" {
  type        = number
  default     = 2.0
}

variable "threshold_backlog" {
  type        = number
  default     = 100000
}

variable "threshold_oldest_age_s" {
  type        = number
  default     = 900
}

variable "threshold_ack_rate_min" {
  type        = number
  default     = 0.1
}

variable "threshold_service_5xx_pct" {
  type        = number
  default     = 1.0
}

variable "threshold_quota_utilization_pct" {
  type        = number
  default     = 80.0
}


//// main.tf
terraform {
  required_version = ">= 1.5.0"
  required_providers {
    google = {
      source  = "hashicorp/google"
      version = ">= 5.29.0"
    }
  }
}

provider "google" {
  project = var.project_id
}

locals {
  topic_regex        = var.topic_regex        # e.g., store-[0-9]+-topic
  subscription_regex = var.subscription_regex # e.g., store-[0-9]+-sub
}

# 1) Publisher — Publish failure rate >= X% (per topic)
resource "google_monitoring_alert_policy" "topic_failure_rate" {
  display_name          = "Pub/Sub Topic - Publish Failure Rate >= ${var.threshold_publish_failure_pct}% (PromQL)"
  combiner              = "OR"
  enabled               = true
  notification_channels = var.notification_channels
  documentation {
    content   = "Failure-rate >= ${var.threshold_publish_failure_pct}% on ${labels.topic_id}. Check publisher errors, quotas, and networking."
    mime_type = "text/markdown"
  }
  conditions {
    display_name = "Failure rate >= ${var.threshold_publish_failure_pct}%"
    condition_prometheus_query_language {
      query = <<-EOT
        100 *
        sum by (topic_id)(
          rate({__name__="pubsub.googleapis.com/topic/send_message_operation_count",
                response_code="ERROR",
                topic_id=~"${local.topic_regex}"}[5m])
        )
        /
        sum by (topic_id)(
          rate({__name__="pubsub.googleapis.com/topic/send_message_operation_count",
                topic_id=~"${local.topic_regex}"}[5m])
        )
        > ${var.threshold_publish_failure_pct}
      EOT
      duration            = "600s"  # 10m
      evaluation_interval = "60s"
      alert_rule          = "PubSubTopicPublishFailureRate"
      labels              = { severity = "warning" }
    }
  }
  alert_strategy { auto_close = "3600s" }
}

# 2) Publisher — p95 publish latency >= Xs (per topic)
resource "google_monitoring_alert_policy" "topic_p95_latency" {
  display_name          = "Pub/Sub Topic - p95 Publish Latency >= ${var.threshold_publish_p95_latency_s}s (PromQL)"
  combiner              = "OR"
  enabled               = true
  notification_channels = var.notification_channels
  documentation {
    content   = "p95 publish latency high on ${labels.topic_id}. Validate client batching, VPC egress, and regional health."
    mime_type = "text/markdown"
  }
  conditions {
    display_name = "p95 publish latency >= ${var.threshold_publish_p95_latency_s}s"
    condition_prometheus_query_language {
      query = <<-EOT
        histogram_quantile(
          0.95,
          sum by (le, topic_id) (
            rate({__name__="pubsub.googleapis.com/topic/send_message_operation_latency_bucket",
                  topic_id=~"${local.topic_regex}"}[5m])
          )
        ) > ${var.threshold_publish_p95_latency_s}
      EOT
      duration            = "600s"
      evaluation_interval = "60s"
      alert_rule          = "PubSubTopicP95PublishLatency"
      labels              = { severity = "warning" }
    }
  }
  alert_strategy { auto_close = "3600s" }
}

# 3) Subscriber — Backlog >= N (per subscription)
resource "google_monitoring_alert_policy" "sub_backlog" {
  display_name          = "Pub/Sub Subscription - Backlog >= ${var.threshold_backlog} (PromQL)"
  combiner              = "OR"
  enabled               = true
  notification_channels = var.notification_channels
  documentation {
    content   = "Undelivered messages >= ${var.threshold_backlog} on ${labels.subscription_id}. Consumers may be stalled."
    mime_type = "text/markdown"
  }
  conditions {
    display_name = "Backlog >= ${var.threshold_backlog}"
    condition_prometheus_query_language {
      query = <<-EOT
        avg_over_time(
          {__name__="pubsub.googleapis.com/subscription/num_undelivered_messages",
           subscription_id=~"${local.subscription_regex}"}[5m]
        ) > ${var.threshold_backlog}
      EOT
      duration            = "900s"  # 15m
      evaluation_interval = "60s"
      alert_rule          = "PubSubSubscriptionBacklog"
      labels              = { severity = "error" }
    }
  }
  alert_strategy { auto_close = "3600s" }
}

# 4) Subscriber — Oldest message age >= Xs (per subscription)
resource "google_monitoring_alert_policy" "sub_oldest_age" {
  display_name          = "Pub/Sub Subscription - Oldest Message Age >= ${var.threshold_oldest_age_s}s (PromQL)"
  combiner              = "OR"
  enabled               = true
  notification_channels = var.notification_channels
  documentation {
    content   = "Oldest unacked message age >= ${var.threshold_oldest_age_s}s on ${labels.subscription_id}. Check throughput and consumer errors."
    mime_type = "text/markdown"
  }
  conditions {
    display_name = "Oldest unacked message age >= ${var.threshold_oldest_age_s}s"
    condition_prometheus_query_language {
      query = <<-EOT
        avg_over_time(
          {__name__="pubsub.googleapis.com/subscription/oldest_unacked_message_age",
           subscription_id=~"${local.subscription_regex}"}[1m]
        ) > ${var.threshold_oldest_age_s}
      EOT
      duration            = "600s"  # 10m
      evaluation_interval = "60s"
      alert_rule          = "PubSubSubscriptionOldestUnackedAge"
      labels              = { severity = "error" }
    }
  }
  alert_strategy { auto_close = "3600s" }
}

# 5) Subscriber — Ack rate ~ 0 (consumer stalled)
resource "google_monitoring_alert_policy" "sub_ack_rate_low" {
  display_name          = "Pub/Sub Subscription - Ack Rate <= ${var.threshold_ack_rate_min} msg/s (PromQL)"
  combiner              = "OR"
  enabled               = true
  notification_channels = var.notification_channels
  documentation {
    content   = "Ack rate ~0 on ${labels.subscription_id}. The consumer might be down or blocked."
    mime_type = "text/markdown"
  }
  conditions {
    display_name = "Ack rate <= ${var.threshold_ack_rate_min} msg/s"
    condition_prometheus_query_language {
      query = <<-EOT
        avg_over_time(
          rate({__name__="pubsub.googleapis.com/subscription/ack_message_count",
                subscription_id=~"${local.subscription_regex}"}[1m])[10m:]
        ) < ${var.threshold_ack_rate_min}
      EOT
      duration            = "600s"
      evaluation_interval = "60s"
      alert_rule          = "PubSubSubscriptionAckRateLow"
      labels              = { severity = "warning" }
    }
  }
  alert_strategy { auto_close = "3600s" }
}

# 6) Subscriber — DLQ traffic > 0 (per subscription)
resource "google_monitoring_alert_policy" "sub_dlq_rate" {
  display_name          = "Pub/Sub Subscription - DLQ Traffic > 0 (PromQL)"
  combiner              = "OR"
  enabled               = true
  notification_channels = var.notification_channels
  documentation {
    content   = "Messages routed to DLQ on ${labels.subscription_id}. Inspect DLQ topic and consumer exceptions."
    mime_type = "text/markdown"
  }
  conditions {
    display_name = "DLQ traffic > 0"
    condition_prometheus_query_language {
      query = <<-EOT
        sum by (subscription_id)(
          rate({__name__="pubsub.googleapis.com/subscription/dead_letter_message_count",
                subscription_id=~"${local.subscription_regex}"}[1m])
        ) > 0
      EOT
      duration            = "300s" # 5m
      evaluation_interval = "60s"
      alert_rule          = "PubSubSubscriptionDLQTraffic"
      labels              = { severity = "warning" }
    }
  }
  alert_strategy { auto_close = "3600s" }
}

# 7) Service health — Pub/Sub API 5xx >= X% (project-wide)
resource "google_monitoring_alert_policy" "service_5xx" {
  display_name          = "Service Health - Pub/Sub API 5xx >= ${var.threshold_service_5xx_pct}% (PromQL)"
  combiner              = "OR"
  enabled               = true
  notification_channels = var.notification_channels
  documentation {
    content   = "Increased Pub/Sub API 5xx across the project; potential regional/service incident."
    mime_type = "text/markdown"
  }
  conditions {
    display_name = "API 5xx >= ${var.threshold_service_5xx_pct}%"
    condition_prometheus_query_language {
      query = <<-EOT
        100 *
        sum(rate({__name__="serviceruntime.googleapis.com/api/request_count",
                  service="pubsub.googleapis.com",
                  response_code_class="5xx"}[1m]))
        /
        sum(rate({__name__="serviceruntime.googleapis.com/api/request_count",
                  service="pubsub.googleapis.com"}[1m]))
        > ${var.threshold_service_5xx_pct}
      EOT
      duration            = "180s"
      evaluation_interval = "60s"
      alert_rule          = "PubSubService5xxRate"
      labels              = { severity = "warning" }
    }
  }
  alert_strategy { auto_close = "3600s" }
}

# 8) Quotas — utilization >= X% (clone for subs & storage)
resource "google_monitoring_alert_policy" "quota_topics" {
  display_name          = "Quota - Topic Count Utilization >= ${var.threshold_quota_utilization_pct}% (PromQL)"
  combiner              = "OR"
  enabled               = true
  notification_channels = var.notification_channels
  documentation {
    content   = "Topic count usage >= ${var.threshold_quota_utilization_pct}% of limit. Consider sharding or quota increase."
    mime_type = "text/markdown"
  }
  conditions {
    display_name = "Topic quota utilization >= ${var.threshold_quota_utilization_pct}%"
    condition_prometheus_query_language {
      query = <<-EOT
        100 *
        avg_over_time({__name__="serviceruntime.googleapis.com/quota/allocation/usage",
                       service="pubsub.googleapis.com",
                       quota_metric="pubsub.googleapis.com/topic_count"}[5m])
        /
        avg_over_time({__name__="serviceruntime.googleapis.com/quota/allocation/limit",
                       service="pubsub.googleapis.com",
                       quota_metric="pubsub.googleapis.com/topic_count"}[5m])
        > ${var.threshold_quota_utilization_pct}
      EOT
      duration            = "600s"
      evaluation_interval = "60s"
      alert_rule          = "PubSubQuotaTopicUtilization"
      labels              = { severity = "warning" }
    }
  }
  alert_strategy { auto_close = "3600s" }
}

resource "google_monitoring_alert_policy" "quota_subs" {
  display_name          = "Quota - Subscription Count Utilization >= ${var.threshold_quota_utilization_pct}% (PromQL)"
  combiner              = "OR"
  enabled               = true
  notification_channels = var.notification_channels
  documentation { content = "Subscription count usage near limit."; mime_type = "text/markdown" }
  conditions {
    display_name = "Subscription quota utilization"
    condition_prometheus_query_language {
      query = <<-EOT
        100 *
        avg_over_time({__name__="serviceruntime.googleapis.com/quota/allocation/usage",
                       service="pubsub.googleapis.com",
                       quota_metric="pubsub.googleapis.com/subscription_count"}[5m])
        /
        avg_over_time({__name__="serviceruntime.googleapis.com/quota/allocation/limit",
                       service="pubsub.googleapis.com",
                       quota_metric="pubsub.googleapis.com/subscription_count"}[5m])
        > ${var.threshold_quota_utilization_pct}
      EOT
      duration            = "600s"
      evaluation_interval = "60s"
      alert_rule          = "PubSubQuotaSubscriptionUtilization"
      labels              = { severity = "warning" }
    }
  }
  alert_strategy { auto_close = "3600s" }
}

resource "google_monitoring_alert_policy" "quota_storage" {
  display_name          = "Quota - Message Storage Utilization >= ${var.threshold_quota_utilization_pct}% (PromQL)"
  combiner              = "OR"
  enabled               = true
  notification_channels = var.notification_channels
  documentation { content = "Message storage bytes near limit; consider retention tuning and cleanup."; mime_type = "text/markdown" }
  conditions {
    display_name = "Storage quota utilization"
    condition_prometheus_query_language {
      query = <<-EOT
        100 *
        avg_over_time({__name__="serviceruntime.googleapis.com/quota/allocation/usage",
                       service="pubsub.googleapis.com",
                       quota_metric="pubsub.googleapis.com/message_storage_bytes"}[5m])
        /
        avg_over_time({__name__="serviceruntime.googleapis.com/quota/allocation/limit",
                       service="pubsub.googleapis.com",
                       quota_metric="pubsub.googleapis.com/message_storage_bytes"}[5m])
        > ${var.threshold_quota_utilization_pct}
      EOT
      duration            = "600s"
      evaluation_interval = "60s"
      alert_rule          = "PubSubQuotaStorageUtilization"
      labels              = { severity = "warning" }
    }
  }
  alert_strategy { auto_close = "3600s" }
}


/// 
